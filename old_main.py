# Bu old_main dosyasÄ± aktif olarak Ã§alÄ±ÅŸtÄ±rÄ±lmamaktadÄ±r. Projenin prototipi iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
# Embedding, cross ranking ve llm modeli bu kod iÃ§erisinde kontrol edilebilir.

import os
import sys
import json
from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import CrossEncoder
from langchain_community.llms import LlamaCpp
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

# Hacettepe Ãœniversitesi EÄŸitim YÃ¶netmeliÄŸi iÃ§in RAG uygulamasÄ±
# Bu kod, Hacettepe Ãœniversitesi EÄŸitim YÃ¶netmeliÄŸi belgelerini kullanarak
# kullanÄ±cÄ±nÄ±n sorularÄ±nÄ± yanÄ±tlamak iÃ§in bir RAG (Retrieval-Augmented Generation) uygulamasÄ±dÄ±r.
# Bu uygulama, belgeleri indeksler ve yanÄ±tlar oluÅŸturur.

# Yollar
load_dotenv()
json_path = os.getenv("JSON_PATH")
faiss_index_path = os.getenv("FAISS_INDEX_PATH")
model_path = os.getenv("MODEL_PATH")
embedding_model_name = os.getenv("EMBEDDING_MODEL")
reranker_model_name = os.getenv("RERANKER_MODEL")

# JSON dosyasÄ± var mÄ± kontrol et
try:    
    with open(json_path, "r", encoding="utf-8") as f:
        # JSON dosyasÄ±nÄ± oku
        raw_data = json.load(f)
    print("âœ… JSON dosyasÄ± baÅŸarÄ±yla yÃ¼klendi.")
except FileNotFoundError:
    print(f"âŒ HATA: JSON dosyasÄ± bulunamadÄ±: {json_path}")
    sys.exit(1)
except json.JSONDecodeError as e:
    print(f"âŒ HATA: JSON bozuk veya geÃ§ersiz: {e}")
    sys.exit(1)

# # # FAISS dizini varsa sil
# # if os.path.exists(faiss_index_path):
# #     try:
# #         # FAISS dizinini sil
# #         shutil.rmtree(faiss_index_path)
# #         print(f"ğŸ—‘ï¸ Var olan FAISS index silindi: {faiss_index_path}")
# #     except Exception as e:
# #         # FAISS dizini silinemediÄŸinde hata ver
# #         print(f"âŒ HATA: FAISS index silinemedi: {e}")
# #         sys.exit(1)

# HuggingFaceEmbeddings ile Embedding modelini yÃ¼kle
try:
    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)
    print(f"âœ… Embedding modeli yÃ¼klendi: {embedding_model_name}")
except Exception as e:
    print(f"âŒ HATA: Embedding modeli yÃ¼klenemedi: {e}")
    sys.exit(1)

# Text splitter (gerekirse)
splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)

# Belgeleri hazÄ±rla ("passage: ..." formatÄ±nda)
documents = []
# Her bir belge iÃ§in iÃ§erik ve metadata al
for item in raw_data:
    content = item.get("content", "").strip()
    # EÄŸer iÃ§erik yoksa geÃ§
    if not content:
        continue
    # Ä°Ã§eriÄŸi kÃ¼Ã§Ã¼k harfe Ã§evir ve bÃ¶l
    chunks = splitter.split_text(content.lower()) 
    # Her bir parÃ§a iÃ§in Document nesnesi oluÅŸtur
    for chunk in chunks:
        documents.append(
            Document(
                # "passage: ..." formatÄ±nda iÃ§erik
                page_content=f"passage: {chunk.lower()}",
                metadata={
                    # Metadata bilgileri
                    "article_id": item.get("article_id", ""),   
                    "section": item.get("section", ""), 
                    "title": item.get("title", "")
                }
            )
        )

# FAISS index Ã¶nceden varsa yeniden oluÅŸturma
if os.path.exists(faiss_index_path):
    print(f"â„¹ï¸ FAISS index zaten var, yeniden oluÅŸturulmadÄ±: {faiss_index_path}")
    try:
        # FAISS dizinini yÃ¼kle ve retriever olarak ayarla
        db = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)   
        retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 20})
        print("âœ… FAISS index yÃ¼klendi ve retriever hazÄ±r.")
    except Exception as e:
        print(f"âŒ HATA: Mevcut FAISS index yÃ¼klenemedi: {e}")
        sys.exit(1)
else:

    print("ğŸ“¦ FAISS index bulunamadÄ±, yeniden oluÅŸturuluyor...")
    try:
        # FAISS dizinini oluÅŸtur, kaydet ve retriever olarak ayarla
        db = FAISS.from_documents(documents, embeddings)
        db.save_local(faiss_index_path)
        retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 20})
        print(f"âœ… FAISS index baÅŸarÄ±yla oluÅŸturuldu, kaydedildi ve retriever hazÄ±r.")
    except Exception as e:
        print(f"âŒ HATA: FAISS index oluÅŸturulamadÄ±: {e}")
        sys.exit(1)

# Reranker modelini yÃ¼kle
try:
    reranker = CrossEncoder(reranker_model_name, device="cpu")
    print(f"âœ… Reranker modeli yÃ¼klendi: {reranker_model_name}")
except Exception as e:
    print(f"âŒ HATA: Reranker modeli yÃ¼klenemedi: {e}")
    sys.exit(1)

# Rerank fonksiyonu
def rerank_documents(query, documents, top_n=10):
    # Rerank iÃ§in belgeleri ve sorguyu uygun formata getir
    pairs = [(query.lower(), doc.page_content.lower()) for doc in documents]
    scores = reranker.predict(pairs)
    # Belgeleri skorlara gÃ¶re sÄ±rala
    reranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in reranked[:top_n]]

# # Ã–rnek Soru Al ve Rerank Et (streamlit_app.py de)
# query = input("\nğŸ“ Soru (Ã§Ä±kmak iÃ§in q): ").strip().lower()
# if query.lower() in ["q", "quit", "exit"]:
#     sys.exit(0)

# FormatÄ± embedding modeline uygun hale getir (streamlit_app.py de)
# formatted_query = f"query: {query.lower()}"  

# # FAISS'ten istediÄŸimiz sayÄ±da belgeyi getir ve kontrol et   (streamlit_app.py de)
# try:
#     retrieved_docs = retriever.invoke(formatted_query)
#     # retrieved_docs = retriever.get_relevant_documents(formatted_query)
#     if not retrieved_docs:
#         print("âŒ HATA: FAISS'ten belge alÄ±namadÄ± (BoÅŸ sonuÃ§ dÃ¶ndÃ¼).")
#         sys.exit(1)
# except Exception as e:
#     print(f"âŒ HATA: FAISS sorgusu sÄ±rasÄ±nda hata oluÅŸtu: {e}")
#     sys.exit(1)

# # Kontrol iÃ§in FAISS output belgelerini yazdÄ±r
# print(f"\nğŸ” FAISS ilk {len(retrieved_docs)} belge getirdi.")
# print("\nğŸ“˜ E5-Large FAISS 20 belge :\n")
# for i, doc in enumerate(retrieved_docs, 1):
#     print(f"{i}. {doc.metadata.get('article_id', '')} - {doc.metadata.get('title', '')}")
#     print(doc.page_content[:350] + "...\n")

# Rerank ile en iyi 10 sÄ±rala (streamlit_app.py de)
# top_docs = rerank_documents(formatted_query, retrieved_docs, top_n=10)

# # En iyi belgeleri yazdÄ±r
# print("\nğŸ“˜ En iyi 10 belge (RERANKED):\n")
# for i, doc in enumerate(top_docs, 1):
#     # Her bir belgenin baÅŸlÄ±ÄŸÄ±nÄ± ve iÃ§eriÄŸini yazdÄ±r
#     print(f"{i}. {doc.metadata.get('article_id', '')} - {doc.metadata.get('title', '')}")
#     print(doc.page_content[:250] + "...\n")

# Gemma modeli (LlamaCpp) ayarlarÄ±
llm = LlamaCpp(
    model_path=model_path,  
    temperature=0.1,    
    max_tokens=512, 
    n_ctx=8192, 
    n_threads=8,
    n_gpu_layers=20,
    top_p=0.9,
    verbose=False
)

# Prompt ÅŸablonu
prompt_template = """Sen Hacettepe Ãœniversitesi iÃ§in uzmanlaÅŸmÄ±ÅŸ bir asistansÄ±n. 
Tek gÃ¶revin, Hacettepe Ãœniversitesi Ã–n Lisans, Lisans EÄŸitimâ€“Ã–ÄŸretim YÃ¶netmeliÄŸi hakkÄ±nda SAÄLANAN BAÄLAMA dayanarak sorularÄ± yanÄ±tlamaktÄ±r.
Soruyu yanÄ±tlamak iÃ§in aÅŸaÄŸÄ±daki alÄ±nmÄ±ÅŸ baÄŸlam parÃ§alarÄ±nÄ± kullan.
EÄŸer soru Hacettepe Ãœniversitesi yÃ¶netmeliÄŸi ile ilgili deÄŸilse (Ã¶rneÄŸin spor, politika, siyaset, ekonomi, genel bilgi, yÃ¶netmelik baÄŸlamÄ± dÄ±ÅŸÄ±ndaki selamlamalar),
bu konuyla yardÄ±mcÄ± olamayacaÄŸÄ±nÄ± ve yalnÄ±zca saÄŸlanan belgelere dayanarak Hacettepe Ãœniversitesi yÃ¶netmeliÄŸi hakkÄ±nda bilgi verebileceÄŸini belirt.
EÄŸer saÄŸlanan baÄŸlam yÃ¶netmelikle ilgili bir sorunun cevabÄ±nÄ± iÃ§ermiyorsa, bilginin saÄŸlanan belgelerde bulunmadÄ±ÄŸÄ±nÄ± belirt.
KullanÄ±cÄ± baÅŸka bir dilde sormadÄ±ÄŸÄ± sÃ¼rece TÃ¼rkÃ§e yanÄ±t ver. Kesin ol ve baÄŸlamdaki bilgilere sadÄ±k kal. DetaylÄ± ve aÃ§Ä±klayÄ±cÄ± ol.
Soru: {question}
=========
{context}
=========
Cevap:"""

# # Prompt ÅŸablonu (Openchat ile uyumlu)
# prompt_template = """<|system|>propmt<|end|>
# <|user|>
# Soru: {question}
# BaÄŸlam:{context}
# <|end|>
# <|assistant|>""" 

# # prompt tanÄ±mÄ±   (alternatif)
# prompt = PromptTemplate.from_template("Soru: {question}\nCevap:")

# PromptTemplate ile ÅŸablonu oluÅŸtur
prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["question", "context"]
)

# # LLMChain ile zinciri oluÅŸtur (alternatif)
# chain = LLMChain(llm=llm, prompt=prompt)

# LLMChain ile zinciri oluÅŸtur
rag_chain = prompt | llm

# SeÃ§ilen top 10 dokÃ¼mandan context hazÄ±rla (streamlit_app.py de)
# context = "\n\n".join([doc.page_content for doc in top_docs])

# LLM'e gÃ¶nder ve cevap al (alternatif)
# response = chain.invoke({
#     "question": query,
#     "context": context
# })

# LLM'e gÃ¶nder ve cevap al (streamlit_app.py de)
# response = rag_chain.invoke({
#     "question": query.lower(),
#     "context": context.lower()
# })

# # CevabÄ± yazdÄ±r (streamlit_app.py de)
# print("\nğŸ§  DeerBot'dan Cevap:\n")
# print(response)