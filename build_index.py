# Proje sadece Main.py veya start.bat dosyalarÄ±ndan Ã§alÄ±ÅŸtÄ±rÄ±labilir.

# build_index.py    
# Hacettepe Ãœniversitesi AsistanÄ± iÃ§in FAISS index oluÅŸturma
# Bu kod, Hacettepe Ãœniversitesi EÄŸitim YÃ¶netmeliÄŸi belgelerini kullanarak
# belgeleri parÃ§alayÄ±p FAISS index oluÅŸturur.


import os
import json
import sys
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

# Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

# Gerekli yollar ve model adÄ±

def build_index(faiss_index_path, json_path, embedding_model_name):
    # JSON dosyasÄ±nÄ± oku
    try:    
        with open(json_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
        print("âœ… JSON dosyasÄ± baÅŸarÄ±yla yÃ¼klendi.")
    except FileNotFoundError:
        print(f"âŒ HATA: JSON dosyasÄ± bulunamadÄ±: {json_path}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ HATA: JSON bozuk veya geÃ§ersiz: {e}")
        sys.exit(1)

    # Embedding modelini yÃ¼kle
    try:
        embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)
        print(f"âœ… Embedding modeli yÃ¼klendi: {embedding_model_name}")
    except Exception as e:
        print(f"âŒ HATA: Embedding modeli yÃ¼klenemedi: {e}")
        sys.exit(1)

    # Text splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )

    # Belgeleri oluÅŸtur
    documents = []
    for item in raw_data:
        if isinstance(item, dict):
            for key, value in item.items():
                if isinstance(value, str):
                    documents.append(Document(page_content=value, metadata={"source": key}))
                elif isinstance(value, list):
                    for sub_item in value:
                        if isinstance(sub_item, str):
                            documents.append(Document(page_content=sub_item, metadata={"source": key}))

    # Belgeleri bÃ¶l
    docs = text_splitter.split_documents(documents)

    # FAISS index'i oluÅŸtur
    db = FAISS.from_documents(docs, embeddings)
    
    # FAISS index'i kaydet
    db.save_local(faiss_index_path)
    
    print(f"âœ… FAISS index baÅŸarÄ±yla oluÅŸturuldu ve kaydedildi: {faiss_index_path}")
    # JSON dosyasÄ±nÄ± yÃ¼kle
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
        print("âœ… JSON dosyasÄ± yÃ¼klendi.")
    except Exception as e:
        print(f"âŒ JSON dosyasÄ± yÃ¼klenemedi: {e}")
        exit(1)


# # # FAISS dizini varsa sil
# # if os.path.exists(faiss_index_path):
# #     try:
# #         # FAISS dizinini sil
# #         shutil.rmtree(faiss_index_path)
# #         print(f"ğŸ—‘ï¸ Var olan FAISS index silindi: {faiss_index_path}")
# #     except Exception as e:
# #         # FAISS dizini silinemediÄŸinde hata ver
# #         print(f"âŒ HATA: FAISS index silinemedi: {e}")
# #         sys.exit(1)

# Embedding modeli yÃ¼kle
    try:
        embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)
        print(f"âœ… Embedding modeli yÃ¼klendi: {embedding_model_name}")
    except Exception as e:
        print(f"âŒ Embedding yÃ¼kleme hatasÄ±: {e}")
        exit(1)

    # Text splitter
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)

    # Belgeleri parÃ§alayÄ±p hazÄ±rla
    documents = []
    for item in raw_data:
        content = item.get("content", "").strip()
        if not content:
            continue
        chunks = splitter.split_text(content.lower())
        for chunk in chunks:
            documents.append(Document(
                page_content=f"passage: {chunk}",
                metadata={
                    "article_id": item.get("article_id", ""),
                    "section": item.get("section", ""),
                    "title": item.get("title", "")
                }
            ))

    print(f"ğŸ”§ Toplam {len(documents)} belge oluÅŸturuldu.")

    # FAISS index oluÅŸtur
    if os.path.exists(faiss_index_path):
        print(f"â„¹ï¸ FAISS index zaten var, yeniden oluÅŸturulmadÄ±: {faiss_index_path}")
        try:
            # FAISS dizinini yÃ¼kle ve retriever olarak ayarla
            db = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)   
            retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 20})
            print("âœ… FAISS index yÃ¼klendi ve retriever hazÄ±r.")
        except Exception as e:
            print(f"âŒ HATA: Mevcut FAISS index yÃ¼klenemedi: {e}")
            sys.exit(1)
    else:

        print("ğŸ“¦ FAISS index bulunamadÄ±, yeniden oluÅŸturuluyor...")
        try:
            # FAISS dizinini oluÅŸtur, kaydet ve retriever olarak ayarla
            db = FAISS.from_documents(documents, embeddings)
            db.save_local(faiss_index_path)
            retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 20})
            print(f"âœ… FAISS index baÅŸarÄ±yla oluÅŸturuldu, kaydedildi ve retriever hazÄ±r.")
        except Exception as e:
            print(f"âŒ HATA: FAISS index oluÅŸturulamadÄ±: {e}")
            sys.exit(1)